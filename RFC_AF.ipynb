{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      correct/anomalous          ra        dec  parallax      pmra     pmdec  \\\n",
      "0                     1  219.951395 -60.972606  0.368673 -2.559203 -2.056463   \n",
      "1                     1  219.790777 -60.981536       NaN       NaN       NaN   \n",
      "2                     1  219.803709 -60.975396       NaN       NaN       NaN   \n",
      "3                     1  219.778267 -60.976624       NaN       NaN       NaN   \n",
      "4                     1  219.785612 -60.974524       NaN       NaN       NaN   \n",
      "...                 ...         ...        ...       ...       ...       ...   \n",
      "7401                  1   28.845468  89.462600       NaN       NaN       NaN   \n",
      "7402                  1   27.815499  89.479682  0.360444 -0.155736  1.833010   \n",
      "7403                  1   27.681935  89.472701  0.724468  6.799206 -1.938184   \n",
      "7404                  1   30.878262  89.482899  0.183544  3.281337  0.156618   \n",
      "7405                  1   31.594724  89.483904  0.158085 -3.238124  3.092111   \n",
      "\n",
      "      astrometric_excess_noise      ruwe  phot_g_mean_mag  \\\n",
      "0                     0.334988  0.860546        16.469640   \n",
      "1                     3.933758       NaN        21.132620   \n",
      "2                     0.000000       NaN        21.190882   \n",
      "3                     4.193961       NaN        21.050072   \n",
      "4                     3.880906       NaN        20.967726   \n",
      "...                        ...       ...              ...   \n",
      "7401                 23.058962       NaN        21.568280   \n",
      "7402                  0.000000  0.983624        19.845778   \n",
      "7403                  0.225780  0.978518        19.312109   \n",
      "7404                  1.261585  1.038276        20.457355   \n",
      "7405                  1.266217  1.118461        19.324654   \n",
      "\n",
      "      phot_bp_rp_excess_factor     bp_rp  \n",
      "0                     1.510628  3.155283  \n",
      "1                     1.599961  1.749922  \n",
      "2                          NaN       NaN  \n",
      "3                     1.467885  1.568964  \n",
      "4                          NaN       NaN  \n",
      "...                        ...       ...  \n",
      "7401                  6.613431  1.866467  \n",
      "7402                  1.230336  0.774540  \n",
      "7403                  1.419945  2.355076  \n",
      "7404                  1.456104  1.676903  \n",
      "7405                  1.305445  1.481434  \n",
      "\n",
      "[7406 rows x 11 columns]\n",
      "Length of the dataset:  7406\n"
     ]
    }
   ],
   "source": [
    "# LOAD AND PREPROCESS THE DATA\n",
    "# Load the dataset\n",
    "\n",
    "file_paths = glob.glob('Data/CSV/*.csv')\n",
    "\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Iterate through the list of file paths and read each CSV into a DataFrame\n",
    "for file_path in file_paths:\n",
    "    df = pd.read_csv(file_path, sep=';')\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all the DataFrames into one\n",
    "concatenated_df = pd.concat(dfs, ignore_index=True) \n",
    "\n",
    "print(concatenated_df)\n",
    "\n",
    "length_df = len(concatenated_df)\n",
    "print(\"Length of the dataset: \", length_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the cleaned dataset:  6051\n"
     ]
    }
   ],
   "source": [
    "# PREPROCESSING\n",
    "df_cleaned = concatenated_df.dropna()\n",
    "\n",
    "length_df_cleaned = len(df_cleaned)\n",
    "print(\"Length of the cleaned dataset: \", length_df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_parallax_errors(df, anomaly_rate, error_factor):\n",
    "    \"\"\"\n",
    "    Augments the parallax values with errors to simulate anomalies.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): A DataFrame containing the original Gaia data.\n",
    "    anomaly_rate (float): The fraction of total objects that should be anomalous.\n",
    "    error_factor (float): A factor that determines the magnitude of the error introduced.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A DataFrame with the augmented parallax data.\n",
    "    \"\"\"\n",
    "    # Make a copy of the DataFrame to avoid altering the original data\n",
    "    augmented_df = df.copy()\n",
    "    \n",
    "    # Calculate the number of objects to alter based on the anomaly rate\n",
    "    num_anomalies = int(len(df) * anomaly_rate)\n",
    "    \n",
    "    # Select random indices for introducing anomalies\n",
    "    anomaly_indices = np.random.choice(df.index, size=num_anomalies, replace=False)\n",
    "    \n",
    "    # Introduce errors in the parallax values\n",
    "    # The error can be a random value that depends on the error_factor and the current parallax value\n",
    "    augmented_df.loc[anomaly_indices, 'parallax'] += error_factor * np.random.randn(num_anomalies) * augmented_df.loc[anomaly_indices, 'parallax']\n",
    "    \n",
    "    # Mark the augmented objects as anomalous\n",
    "    augmented_df.loc[anomaly_indices, 'correct/anomalous'] = 0  # Assuming 0 represents anomalous in your dataset\n",
    "    \n",
    "    return augmented_df\n",
    "\n",
    "# Assuming you have a DataFrame `gaia_data` with your data\n",
    "# augmented_data = augment_parallax_errors(gaia_data, anomaly_rate=0.1, error_factor=0.5)\n",
    "# print(augmented_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_df = augment_parallax_errors(df_cleaned, anomaly_rate=0.1, error_factor=100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print class distribution\n",
    "def print_class_distribution(y_data):\n",
    "    count_class_0 = sum(y_data == 0)\n",
    "    count_class_1 = sum(y_data == 1)\n",
    "    total = len(y_data)\n",
    "    print(\"Class 0: {:.2f}%, Class 1: {:.2f}%\".format((count_class_0 / total) * 100, (count_class_1 / total) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the augmented dataset into initial training set and unlabeled pool\n",
    "initial_train_size = 50\n",
    "# Randomly sample initial training data\n",
    "initial_train_indices = np.random.choice(a=augmented_df.index, size=initial_train_size, replace=False)\n",
    "X_initial = augmented_df.drop('correct/anomalous', axis=1).loc[initial_train_indices]\n",
    "y_initial = augmented_df['correct/anomalous'].loc[initial_train_indices]\n",
    "# Create the pool by excluding the initial training data\n",
    "X_pool = augmented_df.drop('correct/anomalous', axis=1).drop(initial_train_indices)\n",
    "y_pool = augmented_df['correct/anomalous'].drop(initial_train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-25 {color: black;}#sk-container-id-25 pre{padding: 0;}#sk-container-id-25 div.sk-toggleable {background-color: white;}#sk-container-id-25 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-25 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-25 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-25 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-25 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-25 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-25 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-25 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-25 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-25 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-25 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-25 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-25 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-25 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-25 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-25 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-25 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-25 div.sk-item {position: relative;z-index: 1;}#sk-container-id-25 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-25 div.sk-item::before, #sk-container-id-25 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-25 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-25 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-25 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-25 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-25 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-25 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-25 div.sk-label-container {text-align: center;}#sk-container-id-25 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-25 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-25\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" checked><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize and train the Random Forest Classifier\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_initial, y_initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    " # # TODO\n",
    "        # -  make a graph that shows which combination of epochs and nr of datapoints gives the best result\n",
    "        # -  make a graph that shows the accuracy of the model, using the above combination, each time with different number of epochs \n",
    "        # -  zorgt een acquisition function ervoor dat de training data meer gebalanced wordt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 1 nr of datapoints 6000.0\n",
      "Class 0: 9.27%, Class 1: 90.73%\n",
      "epochs 50 nr of datapoints 120.0\n",
      "Class 0: 68.82%, Class 1: 31.18%\n",
      "epochs 100 nr of datapoints 60.0\n",
      "Class 0: 55.45%, Class 1: 44.55%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[323], line 43\u001b[0m\n\u001b[1;32m     37\u001b[0m y_train_extended \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([y_train, most_uncertain_labels])\n\u001b[1;32m     39\u001b[0m \u001b[39m# Print class distribution\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39m# print_class_distribution(y_train_extended)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[39m# Retrain the model on the extended training set\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m rfc\u001b[39m.\u001b[39;49mfit(X_train_extended, y_train_extended)\n\u001b[1;32m     45\u001b[0m \u001b[39m# Update the pool by excluding the most uncertain points\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39m# Use boolean indexing for this\u001b[39;00m\n\u001b[1;32m     47\u001b[0m mask \u001b[39m=\u001b[39m X_pool\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39misin(most_uncertain_points\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/anaconda3/envs/HLML/lib/python3.9/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/HLML/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:348\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[39mif\u001b[39;00m issparse(y):\n\u001b[1;32m    347\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 348\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    349\u001b[0m     X, y, multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mDTYPE\n\u001b[1;32m    350\u001b[0m )\n\u001b[1;32m    351\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m     sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[0;32m~/anaconda3/envs/HLML/lib/python3.9/site-packages/sklearn/base.py:622\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    620\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    621\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 622\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    623\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/anaconda3/envs/HLML/lib/python3.9/site-packages/sklearn/utils/validation.py:1146\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1142\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1143\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1144\u001b[0m     )\n\u001b[0;32m-> 1146\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   1147\u001b[0m     X,\n\u001b[1;32m   1148\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[1;32m   1149\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[1;32m   1150\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   1151\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[1;32m   1152\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m   1153\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[1;32m   1154\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[1;32m   1155\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[1;32m   1156\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[1;32m   1157\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[1;32m   1158\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m   1159\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1160\u001b[0m )\n\u001b[1;32m   1162\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[1;32m   1164\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/anaconda3/envs/HLML/lib/python3.9/site-packages/sklearn/utils/validation.py:784\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    781\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mis_sparse\u001b[39m(dtype):\n\u001b[1;32m    782\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39misinstance\u001b[39m(dtype, SparseDtype)\n\u001b[0;32m--> 784\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(array, \u001b[39m\"\u001b[39m\u001b[39msparse\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39;49mdtypes\u001b[39m.\u001b[39;49mapply(is_sparse)\u001b[39m.\u001b[39many():\n\u001b[1;32m    785\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    786\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mpandas.DataFrame with sparse columns found.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    787\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mIt will be converted to a dense numpy array.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    788\u001b[0m         )\n\u001b[1;32m    790\u001b[0m dtypes_orig \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(array\u001b[39m.\u001b[39mdtypes)\n",
      "File \u001b[0;32m~/anaconda3/envs/HLML/lib/python3.9/site-packages/pandas/core/series.py:4753\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4625\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4626\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4627\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4632\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4633\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4634\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4635\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4636\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4751\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4752\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4753\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\n\u001b[1;32m   4754\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   4755\u001b[0m         func,\n\u001b[1;32m   4756\u001b[0m         convert_dtype\u001b[39m=\u001b[39;49mconvert_dtype,\n\u001b[1;32m   4757\u001b[0m         by_row\u001b[39m=\u001b[39;49mby_row,\n\u001b[1;32m   4758\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   4759\u001b[0m         kwargs\u001b[39m=\u001b[39;49mkwargs,\n\u001b[1;32m   4760\u001b[0m     )\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m~/anaconda3/envs/HLML/lib/python3.9/site-packages/pandas/core/apply.py:1207\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1204\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_compat()\n\u001b[1;32m   1206\u001b[0m \u001b[39m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1207\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/anaconda3/envs/HLML/lib/python3.9/site-packages/pandas/core/apply.py:1287\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[39m# row-wise access\u001b[39;00m\n\u001b[1;32m   1282\u001b[0m \u001b[39m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m \u001b[39m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m \u001b[39m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m \u001b[39m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m action \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj\u001b[39m.\u001b[39mdtype, CategoricalDtype) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1287\u001b[0m mapped \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_map_values(\n\u001b[1;32m   1288\u001b[0m     mapper\u001b[39m=\u001b[39;49mcurried, na_action\u001b[39m=\u001b[39;49maction, convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype\n\u001b[1;32m   1289\u001b[0m )\n\u001b[1;32m   1291\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1292\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1294\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/anaconda3/envs/HLML/lib/python3.9/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[39mreturn\u001b[39;00m arr\u001b[39m.\u001b[39mmap(mapper, na_action\u001b[39m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[39mreturn\u001b[39;00m algorithms\u001b[39m.\u001b[39;49mmap_array(arr, mapper, na_action\u001b[39m=\u001b[39;49mna_action, convert\u001b[39m=\u001b[39;49mconvert)\n",
      "File \u001b[0;32m~/anaconda3/envs/HLML/lib/python3.9/site-packages/pandas/core/algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1812\u001b[0m values \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   1813\u001b[0m \u001b[39mif\u001b[39;00m na_action \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1814\u001b[0m     \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39;49mmap_infer(values, mapper, convert\u001b[39m=\u001b[39;49mconvert)\n\u001b[1;32m   1815\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1816\u001b[0m     \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1817\u001b[0m         values, mapper, mask\u001b[39m=\u001b[39misna(values)\u001b[39m.\u001b[39mview(np\u001b[39m.\u001b[39muint8), convert\u001b[39m=\u001b[39mconvert\n\u001b[1;32m   1818\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# UNCERTAINTY SAMPLING\n",
    "# Active learning loop (acquisition function)\n",
    "# Example: Adding 350 most uncertain points from the pool to the training set\n",
    "total_data_points = 6000\n",
    "augmented_df = augment_parallax_errors(df_cleaned, anomaly_rate=0.1, error_factor=100.0)\n",
    "X_pool_initial = augmented_df.drop('correct/anomalous', axis=1)\n",
    "y_pool_initial = augmented_df['correct/anomalous']\n",
    "\n",
    "# Dictionary to store the balance of the training set for each combination\n",
    "balance_results = {}\n",
    "\n",
    "# for i in range(1, total_data_points):\n",
    "for i in [1, 50, 100, 250, 500, 1000]:\n",
    "    if total_data_points % i == 0:\n",
    "        X_pool = X_pool_initial.copy()\n",
    "        y_pool = y_pool_initial.copy()\n",
    "        X_train = X_initial.copy()\n",
    "        y_train = y_initial.copy()\n",
    "\n",
    "        for _ in range(i):\n",
    "            # print('epochs', i, 'nr of datapoints', total_data_points/i)\n",
    "            # Predict probabilities on the unlabeled data\n",
    "            probabilities = rfc.predict_proba(X_pool)\n",
    "\n",
    "            # Calculate uncertainty and select the most uncertain data points\n",
    "            ## MAX PROBABILITY\n",
    "            uncertainty = abs(probabilities[:, 1] - 0.5)\n",
    "\n",
    "            ## ENNTORPY\n",
    "            # epsilon = 1e-10  # A small constant\n",
    "            # uncertainty = -np.sum(probabilities * np.log(probabilities + epsilon), axis=1)\n",
    "            \n",
    "            n_most_uncertain =  int(total_data_points/i)  # Number of points to acquire in each iteration\n",
    "            most_uncertain_indices = np.argsort(uncertainty)[-n_most_uncertain:]\n",
    "\n",
    "            # Add these points to the training set\n",
    "            most_uncertain_points = X_pool.iloc[most_uncertain_indices]\n",
    "            most_uncertain_labels = y_pool.iloc[most_uncertain_indices]\n",
    "            X_train_extended = pd.concat([X_train, most_uncertain_points])\n",
    "            y_train_extended = pd.concat([y_train, most_uncertain_labels])\n",
    "\n",
    "            # Print class distribution\n",
    "            # print_class_distribution(y_train_extended)\n",
    "\n",
    "            # Retrain the model on the extended training set\n",
    "            rfc.fit(X_train_extended, y_train_extended)\n",
    "\n",
    "            # Update the pool by excluding the most uncertain points\n",
    "            # Use boolean indexing for this\n",
    "            mask = X_pool.index.isin(most_uncertain_points.index)\n",
    "            X_pool = X_pool[~mask]\n",
    "            y_pool = y_pool[~mask]\n",
    "\n",
    "        # Evaluate the balance of the training set\n",
    "        class_distribution = y_train_extended.value_counts(normalize=True)\n",
    "        print('epochs', i, 'nr of datapoints', total_data_points/i)\n",
    "        print_class_distribution(y_train_extended)\n",
    "\n",
    "        # Calculate the balance score\n",
    "        balance = abs(class_distribution[0] - class_distribution[1])\n",
    "        balance_results[(i, int(total_data_points/i))] = balance\n",
    "\n",
    "# Analyze the balance results to find the best combination\n",
    "best_combination = min(balance_results, key=balance_results.get)\n",
    "print(\"Best combination (Epochs, Data points per epoch):\", best_combination)\n",
    "print(\"Balance score:\", balance_results[best_combination])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a graph that shows which combination of epochs and nr of datapoints gives the best result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 60.00%, Class 1: 40.00%\n",
      "Class 0: 13.64%, Class 1: 86.36%\n",
      "Class 0: 54.55%, Class 1: 45.45%\n",
      "Class 0: 48.18%, Class 1: 51.82%\n",
      "Class 0: 46.36%, Class 1: 53.64%\n",
      "Class 0: 20.00%, Class 1: 80.00%\n",
      "Class 0: 54.55%, Class 1: 45.45%\n",
      "Class 0: 36.36%, Class 1: 63.64%\n",
      "Class 0: 40.00%, Class 1: 60.00%\n",
      "Class 0: 53.64%, Class 1: 46.36%\n",
      "Class 0: 29.09%, Class 1: 70.91%\n",
      "Class 0: 44.55%, Class 1: 55.45%\n",
      "Class 0: 26.36%, Class 1: 73.64%\n",
      "Class 0: 36.36%, Class 1: 63.64%\n",
      "Class 0: 9.09%, Class 1: 90.91%\n",
      "Class 0: 8.18%, Class 1: 91.82%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 6.36%, Class 1: 93.64%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 6.36%, Class 1: 93.64%\n",
      "Class 0: 6.36%, Class 1: 93.64%\n",
      "Class 0: 6.36%, Class 1: 93.64%\n",
      "Class 0: 6.36%, Class 1: 93.64%\n",
      "Class 0: 9.09%, Class 1: 90.91%\n",
      "Class 0: 8.18%, Class 1: 91.82%\n",
      "Class 0: 7.27%, Class 1: 92.73%\n",
      "Class 0: 6.36%, Class 1: 93.64%\n",
      "Class 0: 6.36%, Class 1: 93.64%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 6.36%, Class 1: 93.64%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 6.36%, Class 1: 93.64%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 6.36%, Class 1: 93.64%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 7.27%, Class 1: 92.73%\n",
      "Class 0: 7.27%, Class 1: 92.73%\n",
      "Class 0: 7.27%, Class 1: 92.73%\n",
      "Class 0: 7.27%, Class 1: 92.73%\n",
      "Class 0: 10.00%, Class 1: 90.00%\n",
      "Class 0: 6.36%, Class 1: 93.64%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 6.36%, Class 1: 93.64%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 6.36%, Class 1: 93.64%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 6.36%, Class 1: 93.64%\n",
      "Class 0: 6.36%, Class 1: 93.64%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 6.36%, Class 1: 93.64%\n",
      "Class 0: 7.27%, Class 1: 92.73%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 6.36%, Class 1: 93.64%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 7.27%, Class 1: 92.73%\n",
      "Class 0: 11.82%, Class 1: 88.18%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 6.36%, Class 1: 93.64%\n",
      "Class 0: 7.27%, Class 1: 92.73%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 6.36%, Class 1: 93.64%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 6.36%, Class 1: 93.64%\n",
      "Class 0: 6.36%, Class 1: 93.64%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n",
      "Class 0: 5.45%, Class 1: 94.55%\n"
     ]
    }
   ],
   "source": [
    "# # UNCERTAINTY SAMPLING\n",
    "# # Active learning loop (acquisition function)\n",
    "# # Example: Adding 350 most uncertain points from the pool to the training set\n",
    "# for _ in range(100):  # Run 7 iterations (50 points each iteration)\n",
    "#     # Predict probabilities on the unlabeled data\n",
    "#     probabilities = rfc.predict_proba(X_pool)\n",
    "\n",
    "#     # Calculate uncertainty and select the most uncertain data points\n",
    "#     uncertainty = abs(probabilities[:, 1] - 0.5)\n",
    "#     n_most_uncertain = 60  # Number of points to acquire in each iteration\n",
    "#     most_uncertain_indices = uncertainty.argsort()[:n_most_uncertain]\n",
    "\n",
    "#     # Add these points to the training set\n",
    "#     most_uncertain_points = X_pool.iloc[most_uncertain_indices]\n",
    "#     most_uncertain_labels = y_pool.iloc[most_uncertain_indices]\n",
    "#     X_train_extended = pd.concat([X_initial, most_uncertain_points])\n",
    "#     y_train_extended = pd.concat([y_initial, most_uncertain_labels])\n",
    "\n",
    "#     # Print class distribution\n",
    "#     print_class_distribution(y_train_extended)\n",
    "\n",
    "#     # Retrain the model on the extended training set\n",
    "#     rfc.fit(X_train_extended, y_train_extended)\n",
    "\n",
    "#     # Update the pool by excluding the most uncertain points\n",
    "#     # Use boolean indexing for this\n",
    "#     mask = X_pool.index.isin(most_uncertain_points.index)\n",
    "#     X_pool = X_pool[~mask]\n",
    "#     y_pool = y_pool[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Active learning loop (acquisition function)\n",
    "# # Example: Adding 350 most uncertain points from the pool to the training set\n",
    "# for _ in range(100):  # Run 7 iterations (50 points each iteration)\n",
    "#     # Predict probabilities on the unlabeled data\n",
    "#     probabilities = rfc.predict_proba(X_pool)\n",
    "\n",
    "#     # Calculate uncertainty and select the most uncertain data points\n",
    "#     uncertainty = abs(probabilities[:, 1] - 0.5)\n",
    "#     n_most_uncertain = 60  # Number of points to acquire in each iteration\n",
    "#     most_uncertain_indices = uncertainty.argsort()[:n_most_uncertain]\n",
    "\n",
    "#     # Add these points to the training set\n",
    "#     most_uncertain_points = X_pool.iloc[most_uncertain_indices]\n",
    "#     most_uncertain_labels = y_pool.iloc[most_uncertain_indices]\n",
    "#     X_train_extended = pd.concat([X_initial, most_uncertain_points])\n",
    "#     y_train_extended = pd.concat([y_initial, most_uncertain_labels])\n",
    "\n",
    "#     # Print class distribution\n",
    "#     print_class_distribution(y_train_extended)\n",
    "\n",
    "#     # Retrain the model on the extended training set\n",
    "#     rfc.fit(X_train_extended, y_train_extended)\n",
    "\n",
    "#     # Update the pool by excluding the most uncertain points\n",
    "#     # Use boolean indexing for this\n",
    "#     mask = X_pool.index.isin(most_uncertain_points.index)\n",
    "#     X_pool = X_pool[~mask]\n",
    "#     y_pool = y_pool[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataset:  921\n",
      "Length of the cleaned dataset:  741\n",
      "Index(['correct/anomalous', 'ra', 'dec', 'parallax', 'pmra', 'pmdec',\n",
      "       'astrometric_excess_noise', 'ruwe', 'phot_g_mean_mag',\n",
      "       'phot_bp_rp_excess_factor', 'bp_rp'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Loading a separate test set from another CSV file\n",
    "test_df = pd.read_csv('Data/Capella5arcmin.csv', sep=';')\n",
    "length_df = len(test_df)\n",
    "print(\"Length of the dataset: \", length_df)\n",
    "\n",
    "df_cleaned = test_df.dropna()\n",
    "length_df_cleaned = len(df_cleaned)\n",
    "print(\"Length of the cleaned dataset: \", length_df_cleaned)\n",
    "\n",
    "column_names = df_cleaned.columns\n",
    "print(column_names)\n",
    "\n",
    "augmented_test_df = augment_parallax_errors(df_cleaned, anomaly_rate=0.5, error_factor=100.0)\n",
    "X_test = augmented_test_df.drop('correct/anomalous', axis=1)\n",
    "y_test = augmented_test_df['correct/anomalous']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5870445344129555\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.17      0.29       370\n",
      "           1       0.55      1.00      0.71       371\n",
      "\n",
      "    accuracy                           0.59       741\n",
      "   macro avg       0.77      0.59      0.50       741\n",
      "weighted avg       0.77      0.59      0.50       741\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the new test set\n",
    "y_pred = rfc.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HLML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
